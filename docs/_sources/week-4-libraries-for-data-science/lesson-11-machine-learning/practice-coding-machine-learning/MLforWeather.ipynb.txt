{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\udea7 Practice: Coding Machine Learning"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["In this notebook, we will practice trying to predict the weather. We won't try to predict it in the sense you are familiar with, where meteorologists try to predict what the weather will be a week out from now. Instead, we will do a simpler example where we look at various information about a day and try to predict the maximimum temperature that day.\n", "\n", "The data is stored in `weather.csv` (remember you should read it as `/home/weather.csv`) and has the following columns.\n", "* `STA`: A code representing what station the measurements were taken from\n", "* `YR`: Which year this measurement was taken\n", "* `MO`: Which month this measurement was taken\n", "* `DA`: Which day this measurement was taken\n", "* `MAX` (our target): The maximum temperature that was reached that day\n", "* `MIN`: The minimum temperature that was reached that day.\n", "\n", "Since the target we want to predict is a number, this will be a regression task rather than a classification task. Almost all the code you will write will be the same as we saw in the lesson, except:\n", "* You will use a `DecisionTreeRegressor` from `sklearn.tree` instead of a `DecisionTreeClassifier`\n", "* You will use the `mean_squared_error` function from the `sklearn.metrics` module instead of `accuracy_score`. It behaves similarly in the sense it takes the true labels and the predicted labels, but is different in that it returns the **error** of the predictions instead. Formally, this is returning the mean-squared error between your predictions and the true values (find the difference for each example, square them, and average them). A higher MSE means the model did worse, while an MSE of 0 means there were no errors!\n", "\n", "Unlike previous notebook practice problems, we only have one cell to test your code at the end. You shouldn't modify that cell at all and you should use the cell(s) above to write your solutions. Remember to import the proper libraries!\n", "\n", "To pass our tests, you will need to use the following variable names for the parts of the problem (you may have other variables, but our tests won't look at them):\n", "* `data` should store the `DataFrame` of all the data stored in `/home/weather.csv`.\n", "* `features` should store the `DataFrame` of just the features.\n", "* `labels` should store the `Series` of labels.\n", "* `model` should store the `DecisionTreeRegressor`.\n", "* `error` should store the error of the trained `model` on the whole dataset. \n", "\n", "We don't specify each step so that you refer back to your notes and the code process you saw from the notebook earlier in the lesson. Refer back to that for the steps to train the model (accounting for the differences we highlighted above). \n", "\n", "As a hint for correctness on this task, your model should get 0 error on this dataset. We will discuss in Lesson 12 why getting 0 error might be a sign of something is actually wrong with our model, but for this lesson, we will consider that correct! "]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": [""]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": [""]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": [""]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": [""]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": [""]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": [""]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["### edTest(test_variables) ###\n", "\n", "# Don't edit this cell!"]}]}