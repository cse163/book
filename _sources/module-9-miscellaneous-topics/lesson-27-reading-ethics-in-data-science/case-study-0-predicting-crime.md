# Case Study 0: Predicting Criminality

## Task

Read [this article](https://www.callingbull.org/case_studies/case_study_criminal_machine_learning.html) written by UW's [Carl Bergstrom](http://ctbergstrom.com/) and [Jevin West](http://www.jevinwest.org/).

## Recap

Data scientists use models to not only make predictions but also to justify some phenomena (e.g., an interpretable machine learning model). While this is a useful endeavor, it can have some downfalls. Make sure you explore alternative hypotheses (maybe simpler ones) that are also likely to explain the phenomena. It would be best if you took the time to reflect on your explanation to check for any implicit biases present (e.g., confirmation bias). While it's hard to spot implicit biases (because they are implicit), taking time to use your [System 2 mode of thought](https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow#Replication_crisis) can help catch them.

It's incredibly important to start any data analysis project by asking who will use its results. Is it possible for someone to use this model to the detriment of other's health, safety, or privacy? In the case of predicting criminality, there are some pretty scary possibilities of people in power abusing a tool like this to algorithmically enforce historic biases.

In general, it can be a bit tricky navigate discussions of risk and value: It's probably the case that you could argue that anyone can use any tool to cause harm, but that doesn't mean we should never make new tools. It can help to think about the negatives in contrast to the value a tool adds to the world (e.g., think about negatives of not using that tools). If there is an excellent argument for value-added, then the calculus on risk-reward will be more difficult to navigate. It's tricky to navigate since different people have different views on injury and value, but having that discussion in the first place is a critical first step.
